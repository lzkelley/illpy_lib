{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# %load init.ipy\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import warnings\n",
    "import glob\n",
    "# from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "# import scipy as sp\n",
    "# import scipy.interpolate\n",
    "# import astropy as ap\n",
    "import h5py\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# --- LZK Libs ---\n",
    "\n",
    "import zcode.math as zmath\n",
    "import zcode.plot as zplot\n",
    "# import zcode.astro as zastro\n",
    "from zcode.constants import *\n",
    "import zcode.inout as zio\n",
    "\n",
    "import kalepy as kale\n",
    "\n",
    "\n",
    "# --- Arepo / Illustris ---\n",
    "\n",
    "import illpy\n",
    "import illpy_lib\n",
    "import illpy_lib.illcosmo\n",
    "import illpy_lib.illbh\n",
    "import illpy_lib.illbh.mergers\n",
    "import illpy_lib.illbh.details\n",
    "import illpy_lib.illbh.snapshots\n",
    "# import illpy_lib.illbh.groupcats\n",
    "\n",
    "from illpy_lib.illbh import BH_TYPE, KEYS\n",
    "\n",
    "\n",
    "# --- Generic Setup ---\n",
    "\n",
    "# Silence annoying numpy errors\n",
    "np.seterr(divide='ignore', invalid='ignore', over='ignore');\n",
    "%precision %.2g\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Plotting settings\n",
    "mpl.rc('font', **{'family': 'serif', 'sans-serif': ['Times'], 'size': 12})\n",
    "mpl.rc('lines', solid_capstyle='round')\n",
    "mpl.rc('mathtext', fontset='cm')\n",
    "plt.rcParams.update({'grid.alpha': 0.5})\n",
    "\n",
    "\n",
    "# --- Project Setup ---\n",
    "\n",
    "# PATH_BASE = \"/blue/lblecha/lkelley/arepo_sims/cosmo_zooms/dynamics-comparison\"\n",
    "# PATH_BASE = \"/Volumes/Smally/arepo_sims/\"\n",
    "\n",
    "# SIM_NAME = \"bh_new_centering\"\n",
    "\n",
    "# sim_path = os.path.join(PATH_BASE, SIM_NAME, 'output', '')\n",
    "\n",
    "# PATH = (\n",
    "# #     \"/Volumes/Smally/arepo_sims/\"\n",
    "# #     \"bh_new_centering/output\"\n",
    "#     \"new-log_pos-test/output\"\n",
    "# #     \"/Volumes/Smally/log_test_data/\"\n",
    "# )\n",
    "\n",
    "# SIM_PATHS = [\n",
    "#     \"bh_new_centering\",\n",
    "#     \"repos-potmin\",\n",
    "#     \"unalt-dyn\"\n",
    "# ]\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_PATH = (\n",
    "#     \"/net/hernquistfs3/hernquistfs3/share_root/IllustrisTNG/Runs/\"\n",
    "#     \"L75n1820TNG/\"\n",
    "    \"/n/holylfs/LABS/hernquist_lab/IllustrisTNG/Runs/\"\n",
    "    \"L75n1820TNG/\"\n",
    ")\n",
    "PROC_PATH = (\n",
    "    \"/n/holystore01/LABS/hernquist_lab/Lab/ghernquist/lkelley/illustris-data/newer/\"\n",
    "    \"L75n1820TNG/\"\n",
    ")\n",
    "NUM_SNAPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found parameters file: '/n/holylfs/LABS/hernquist_lab/IllustrisTNG/Runs/L75n1820TNG/arepo/param.txt-usedvalues'\n",
      "Found 100 snapshot directories\n",
      "Snap 0: loading header info from '/n/holylfs/LABS/hernquist_lab/IllustrisTNG/Runs/L75n1820TNG/output/snapdir_000/snap_000.131.hdf5'\n"
     ]
    }
   ],
   "source": [
    "cosmo = illpy_lib.illcosmo.Simulation_Cosmology(SIM_PATH)\n",
    "# snap_scales_fname = \"snap-scales.npz\"\n",
    "# snap_scales_fname = os.path.join(PROC_PATH, snap_scales_fname)\n",
    "# if not os.path.exists(snap_scales_fname):\n",
    "#     snap_scales = np.zeros(NUM_SNAPS)\n",
    "#     for sn in tqdm.trange(NUM_SNAPS):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`_load()`: recreate: False, exists: True (/n/holystore01/LABS/hernquist_lab/Lab/ghernquist/lkelley/illustris-data/newer/L75n1820TNG/bh-groupcats.hdf5)\n",
      "Loaded    2728779 entries from '/n/holystore01/LABS/hernquist_lab/Lab/ghernquist/lkelley/illustris-data/newer/L75n1820TNG/bh-groupcats.hdf5', created '2021-01-12 17:09:25.118771'\n"
     ]
    }
   ],
   "source": [
    "import illpy_lib.illbh.groupcats\n",
    "from illpy_lib.illbh.groupcats import GCAT_KEYS\n",
    "\n",
    "# gcat_s10 = illpy_lib.illbh.groupcats.Groupcats_Snap(\n",
    "#     10, SIM_PATH, PROC_PATH, recreate=False, verbose=True, load=True,  # , cosmo=cosmo\n",
    "# )\n",
    "\n",
    "gcats = illpy_lib.illbh.groupcats.Groupcats(SIM_PATH, PROC_PATH, recreate=False, verbose=True, load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParticleIDs, SubhaloBHMass, SubhaloBHMdot, SubhaloBfldDisk, SubhaloBfldHalo, SubhaloCM, SubhaloGasMetalFractions, SubhaloGasMetalFractionsHalfRad, SubhaloGasMetalFractionsMaxRad, SubhaloGasMetalFractionsSfr, SubhaloGasMetalFractionsSfrWeighted, SubhaloGasMetallicity, SubhaloGasMetallicityHalfRad, SubhaloGasMetallicityMaxRad, SubhaloGasMetallicitySfr, SubhaloGasMetallicitySfrWeighted, SubhaloGrNr, SubhaloHalfmassRad, SubhaloHalfmassRadType, SubhaloIDMostbound, SubhaloLen, SubhaloLenType, SubhaloMass, SubhaloMassInHalfRad, SubhaloMassInHalfRadType, SubhaloMassInMaxRad, SubhaloMassInMaxRadType, SubhaloMassInRad, SubhaloMassInRadType, SubhaloMassType, SubhaloParent, SubhaloPos, SubhaloSFR, SubhaloSFRinHalfRad, SubhaloSFRinMaxRad, SubhaloSFRinRad, SubhaloSpin, SubhaloStarMetalFractions, SubhaloStarMetalFractionsHalfRad, SubhaloStarMetalFractionsMaxRad, SubhaloStarMetallicity, SubhaloStarMetallicityHalfRad, SubhaloStarMetallicityMaxRad, SubhaloStellarPhotometrics, SubhaloStellarPhotometricsMassInRad, SubhaloStellarPhotometricsRad, SubhaloVel, SubhaloVelDisp, SubhaloVmax, SubhaloVmaxRad, SubhaloWindMass, halo, scale, snap, subhalo, unique_counts, unique_ids, unique_indices\n",
      "halo    =  2.7e+06/2.7e+06 = 0.9999\n",
      "subhalo =  2.7e+06/2.7e+06 = 0.9999\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join(gcats.keys()))\n",
    "print(\"halo    = \", zmath.frac_str(gcats['halo'] >= 0))\n",
    "print(\"subhalo = \", zmath.frac_str(gcats['subhalo'] >= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import illpy_lib.illbh.snapshots\n",
    "# reload(illpy_lib.illbh.snapshots)\n",
    "# snap_99 = illpy_lib.illbh.snapshots.Snapshots_Snap(99, SIM_PATH, PROC_PATH, recreate=True)\n",
    "snaps = illpy_lib.illbh.snapshots.Snapshots_TNG(SIM_PATH, PROC_PATH, recreate=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:WARNING: loaded sim_path '/net/hernquistfs3/hernquistfs3/share_root/IllustrisTNG/Runs/L75n1820TNG' does not match current '/n/holylfs/LABS/hernquist_lab/IllustrisTNG/Runs/L75n1820TNG'!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`_load()`: recreate: False, exists: True (/n/holystore01/LABS/hernquist_lab/Lab/ghernquist/lkelley/illustris-data/newer/L75n1820TNG/bh-mergers.hdf5)\n",
      "Loaded      30972 entries from '/n/holystore01/LABS/hernquist_lab/Lab/ghernquist/lkelley/illustris-data/newer/L75n1820TNG/bh-mergers.hdf5', created '2020-11-17 00:12:58.323026'\n"
     ]
    }
   ],
   "source": [
    "import illpy_lib.illbh.mergers\n",
    "reload(illpy_lib.illbh.mergers)\n",
    "mergers = illpy_lib.illbh.mergers.Mergers_TNG(SIM_PATH, PROC_PATH, recreate=False, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BH_Mass',\n",
       " 'Masses',\n",
       " 'ParticleIDs',\n",
       " 'scale',\n",
       " 'task',\n",
       " 'unique_counts',\n",
       " 'unique_ids',\n",
       " 'unique_indices']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'illpy_lib.illbh.details' from '/n/holystore01/LABS/hernquist_lab/Lab/ghernquist/lkelley/research/arepo/illpy_lib/illpy_lib/illbh/details.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(illpy_lib.illbh)\n",
    "import illpy_lib.illbh.details\n",
    "reload(illpy_lib.illbh.details)\n",
    "# dets_t0 = illpy_lib.illbh.details.Details_TNG_Task(0, SIM_PATH, PROC_PATH, recreate=False, verbose=True, cosmo=cosmo)\n",
    "# dets = illpy_lib.illbh.details.Details_TNG(SIM_PATH, PROC_PATH, recreate=False, verbose=False)\n",
    "# dets_s10 = illpy_lib.illbh.details.Details_TNG_Snap(10, SIM_PATH, PROC_PATH, recreate=False, verbose=True, cosmo=cosmo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Mergers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tree from '/n/holystore01/LABS/hernquist_lab/Lab/ghernquist/lkelley/research/arepo/illpy_lib/notebooks/tree-temp.npz'\n"
     ]
    }
   ],
   "source": [
    "def build_tree(mrgs, start=None):\n",
    "    \"\"\"\n",
    "    The first BH is the \"out\" (remaining) BH, second BH is \"in\" (removed) BH\n",
    "    \"\"\"\n",
    "    tree = dict()\n",
    "    if start is None:\n",
    "        start = 0\n",
    "        \n",
    "    def mstr(mm):\n",
    "        msg = \"{:6d} a={:.8f} task={:3d} \".format(mm, mrgs[KEYS.SCALE][mm], mrgs[KEYS.TASK][mm])\n",
    "        ids = mrgs[KEYS.ID][mm]\n",
    "        mass = mrgs[KEYS.MASS][mm]\n",
    "        bh_mass = mrgs[KEYS.BH_MASS][mm]\n",
    "        for name, tt in zip([' in', 'out'], [BH_TYPE.IN, BH_TYPE.OUT]):\n",
    "            try:\n",
    "                msg += \"{} id={:14d} bh_mass={:.8f} mass={:.8f} \".format(\n",
    "                    name, ids[tt], bh_mass[tt], mass[tt])\n",
    "            except:\n",
    "                print(\"ids  = \", ids[tt], type(ids[tt]))\n",
    "                print(\"mass = \", mass[tt], type(mass[tt]))\n",
    "                raise\n",
    "\n",
    "        return msg\n",
    "\n",
    "    # -- Build Merger Tree\n",
    "\n",
    "    num = mrgs[KEYS.SCALE].size\n",
    "    next = np.ones(num, np.int32) * -1\n",
    "    prev = np.ones((num, 2), np.int32) * -1\n",
    "    snap = np.ones(num, np.int32) * -1\n",
    "    use_bh_mass = True\n",
    "    sn = 0\n",
    "    for mm in tqdm.tqdm(range(start, num), 'mergers'):\n",
    "        mids = mrgs[KEYS.ID][mm]\n",
    "        ma = mrgs[KEYS.SCALE][mm]\n",
    "        while ma > cosmo.scale[sn]:\n",
    "            sn += 1\n",
    "            if sn == cosmo.scale.size:\n",
    "                raise ValueError(f\"Could not find correct snapshot for merger sca {ma:.10f}!\")\n",
    "\n",
    "        snap[mm] = sn\n",
    "        if use_bh_mass:\n",
    "            mass = mrgs[KEYS.BH_MASS][mm]\n",
    "            if np.all(mass == 0.0):\n",
    "                mass = None\n",
    "                logging.warning(\"WARNING: \")\n",
    "        else:\n",
    "            mass = mrgs[KEYS.MASS][mm]\n",
    "\n",
    "        for nn in range(mm+1, num):\n",
    "            nids = mrgs[KEYS.ID][nn]\n",
    "            na = mrgs[KEYS.SCALE][nn]\n",
    "            # Make sure scales are not decreasing (data is already sorted; should be impossible)\n",
    "            if na < ma:\n",
    "                err = f\"ERROR: scale regression: {mm}={ma} ==> {nn}={na}!\"\n",
    "                logging.error(err)\n",
    "                raise ValueError(err)\n",
    "\n",
    "            # make sure 'in' BH is never in another merger\n",
    "            if mids[BH_TYPE.IN] in nids:\n",
    "                err = \"ERROR: 'in' BH from merger {} found in merger {}!\".format(mm, nn)\n",
    "                logging.error(err)\n",
    "                logging.error(mstr(mm))\n",
    "                logging.error(mstr(nn))\n",
    "                raise ValueError(err)\n",
    "\n",
    "            # Check if 'out' BH is in a subsequent merger\n",
    "            if mids[BH_TYPE.OUT] in nids:\n",
    "                next[mm] = nn\n",
    "                mass_next = mrgs[KEYS.MASS][nn]\n",
    "                for jj in range(2):\n",
    "                    if mids[BH_TYPE.OUT] == nids[jj]:\n",
    "                        prev[nn, jj] = mm\n",
    "                        # Make sure masses increased\n",
    "                        if mass_next[jj] < mass[BH_TYPE.OUT]:\n",
    "                            err = \"ERROR: BH mass loss between merger {} and {}\".format(mm, nn)\n",
    "                            logging.error(err)\n",
    "                            logging.error(mstr(mm))\n",
    "                            logging.error(mstr(nn))\n",
    "                            if use_bh_mass:\n",
    "                                raise ValueError(err)\n",
    "                            else:\n",
    "                                logging.error(\"Using `Mass` not `BH_Mass`... continuing!\")\n",
    "\n",
    "                        break  # if\n",
    "\n",
    "                break  # for jj\n",
    "\n",
    "    tree[KEYS.T_NEXT] = next\n",
    "    tree[KEYS.T_PREV] = prev\n",
    "    tree[KEYS.SNAP] = snap\n",
    "\n",
    "    num_next = np.ones(num, np.int32) * -1\n",
    "    num_prev = np.ones((num, 2), np.int32) * -1\n",
    "\n",
    "    def _count_prev(mm):\n",
    "        for bh in range(2):\n",
    "            temp = prev[mm][bh]\n",
    "            if temp >= 0:\n",
    "                num_prev[mm, bh] = _count_prev(temp)\n",
    "            else:\n",
    "                num_prev[mm, bh] = 0\n",
    "\n",
    "        return num_prev[mm].sum() + 1\n",
    "\n",
    "    for mm in reversed(range(num)):\n",
    "        temp = next[mm]\n",
    "        if temp >= 0:\n",
    "            num_next[mm] = num_next[temp] + 1\n",
    "            continue\n",
    "\n",
    "        num_next[mm] = 0\n",
    "        _count_prev(mm)\n",
    "\n",
    "    tree[KEYS.T_NUM_NEXT] = num_next\n",
    "    tree[KEYS.T_NUM_PREV] = num_prev\n",
    "\n",
    "    # -- Report statistics\n",
    "\n",
    "    num_next = (next >= 0)\n",
    "    print(\"Mergers with subsequent mergers: {}\".format(zmath.frac_str(num_next)))\n",
    "    print(\"Mergers that are final remnants: {}\".format(zmath.frac_str(~num_next)))\n",
    "\n",
    "    mult = num_prev[num_prev >= 0]\n",
    "    print(\"Remnant merger-multiplicity:\")\n",
    "    print(\"\\tzero = {}\".format(zmath.frac_str(mult == 0)))\n",
    "    print(\"\\tave  = {:.2f}\".format(mult.mean()))\n",
    "    print(\"\\tmed  = {:.2f}\".format(np.median(mult)))\n",
    "    print(\"\\t{}\".format(zmath.stats_str(mult)))\n",
    "\n",
    "    return tree\n",
    "\n",
    "# RECREATE_TREE = True\n",
    "RECREATE_TREE = False\n",
    "tree_fname = \"tree-temp.npz\"\n",
    "tree_fname = os.path.abspath(tree_fname)\n",
    "tree_file_exists = os.path.exists(tree_fname)\n",
    "if not tree_file_exists or RECREATE_TREE:\n",
    "    print(f\"Tree file '{tree_fname}' exists:{tree_file_exists}!\")\n",
    "    tree = build_tree(mergers)\n",
    "    np.savez(tree_fname, **tree)\n",
    "    print(f\"Saved to '{tree_fname}' size {zio.get_file_size(tree_fname)}\")\n",
    "else:\n",
    "    tree = np.load(tree_fname)\n",
    "    print(f\"Loaded tree from '{tree_fname}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tree(tree, mergers):\n",
    "    tnext = tree['tree_next']\n",
    "    tprev = tree['tree_prev']\n",
    "    tnext_num = tree['tree_num_next']\n",
    "    tprev_num = tree['tree_num_prev']\n",
    "\n",
    "    mids = mergers[KEYS.ID]\n",
    "    bhin = BH_TYPE.IN\n",
    "    bhot = BH_TYPE.OUT\n",
    "    print(f\"IN  = {bhin}\")\n",
    "    print(f\"OUT = {bhot}\")\n",
    "\n",
    "    def mstr(mm):\n",
    "        return f\"{mm} - {mids[mm][0]}, {mids[mm][1]} - next: {tnext[mm]}, prev: {tprev[mm]}\"\n",
    "\n",
    "    for ii, nn in enumerate(tqdm.tqdm(tnext)):\n",
    "        this_mids = mids[ii]\n",
    "        this_in = this_mids[bhin]\n",
    "        this_ot = this_mids[bhot]\n",
    "        err = \"\"\n",
    "        if nn > 0:\n",
    "            if this_ot not in mids[nn]:\n",
    "                err += f\"Out BH of {ii} not in next merger {nn}!\"\n",
    "                raise ValueError()\n",
    "\n",
    "            if this_ot == mids[nn][bhot]:\n",
    "                if tprev[nn][bhot] != ii:\n",
    "                    err += f\"Prev of {nn} out is not {ii}!\"\n",
    "\n",
    "            if this_in == mids[nn][bhin]:\n",
    "                if tprev[nn][bhin] != ii:\n",
    "                    err += f\"Prev of {nn} in  is not {ii}!\"\n",
    "\n",
    "        for jj, pp in enumerate(tprev[ii]):\n",
    "            if pp < 0:\n",
    "                continue\n",
    "            if this_mids[jj] != mids[pp][bhot]:\n",
    "                err += f\"Prev of {ii} does not match this bh {jj}!\"\n",
    "                break\n",
    "        else:\n",
    "            pp = None\n",
    "\n",
    "        if len(err) > 0:      \n",
    "            print(mstr(ii))\n",
    "            if pp is not None:\n",
    "                print(\"prev = \", mstr(pp))\n",
    "            print(\"next = \", mstr(nn))\n",
    "            raise ValueError(err)\n",
    "            \n",
    "    return\n",
    "\n",
    "# check_tree(tree, mergers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble all BH Details Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bhs from '/n/holystore01/LABS/hernquist_lab/Lab/ghernquist/lkelley/research/arepo/illpy_lib/notebooks/bhs-temp.npz' and '/n/holystore01/LABS/hernquist_lab/Lab/ghernquist/lkelley/research/arepo/illpy_lib/notebooks/bhs-meta-temp.npz'\n"
     ]
    }
   ],
   "source": [
    "def select_at_interval(xx, dx):\n",
    "    \"\"\"Select a subset of array elements to achieve a certain average spacing.\n",
    "    \"\"\"\n",
    "    beg = np.min(xx)\n",
    "    end = np.max(xx)\n",
    "    num = (end - beg) / dx\n",
    "    num = np.int(np.ceil(num))\n",
    "    # Array of optimal spacings\n",
    "    yy = beg + dx * np.arange(num+1)    \n",
    "    # Find locations in input nearest to the optimal spacings\n",
    "    sel = zmath.argnearest(xx, yy, assume_sorted=True)\n",
    "    # Also include edges\n",
    "    sel = np.concatenate([[0], sel, [xx.size-1]])\n",
    "    # Make sure results are all unique\n",
    "    sel = np.unique(sel)\n",
    "    return sel\n",
    "\n",
    "def setup_or_expand(data, dets, size):\n",
    "    setup = (data is None)\n",
    "    if setup:\n",
    "        data = dict()\n",
    "    for kk in dets.keys():\n",
    "        if kk in KEYS._DERIVED:\n",
    "            continue\n",
    "        vv = dets[kk]\n",
    "        shp = (size,) + np.shape(vv)[1:]\n",
    "        temp = np.zeros(shp, dtype=vv.dtype)\n",
    "        if setup:\n",
    "            data[kk] = temp\n",
    "        else:\n",
    "            data[kk] = np.concatenate([data[kk], temp])\n",
    "\n",
    "    return data\n",
    "\n",
    "def built_full_bhs(snaps, mergers, verbose=False):\n",
    "    NUM_SNAPS = 100\n",
    "    LAST_SNAP = NUM_SNAPS - 1\n",
    "    TARGET_DELTA_SCA = 1e-3\n",
    "\n",
    "    snaps_uids = snaps[KEYS.U_IDS]\n",
    "    snaps_ulocs = snaps[KEYS.U_INDICES]\n",
    "    snaps_unums = snaps[KEYS.U_COUNTS]\n",
    "    mids = mergers[KEYS.ID]\n",
    "\n",
    "    num_mrgs = mergers[KEYS.SCALE].size\n",
    "    mrg_uids = mergers[KEYS.U_IDS]\n",
    "    # First location (merger) each unique BH participates in (can follow the tree afterwards)\n",
    "    mrg_ulocs = mergers[KEYS.U_INDICES]\n",
    "    mrg_unums = mergers[KEYS.U_COUNTS]\n",
    "\n",
    "    bhs_ids = np.zeros_like(snaps_uids)\n",
    "    bhs_sca_frst = np.zeros(bhs_ids.size)\n",
    "    bhs_sca_last = np.zeros(bhs_ids.size)\n",
    "    bhs_snap_frst = -1 * np.ones(bhs_ids.size, dtype=int)\n",
    "    bhs_snap_last = -1 * np.ones(bhs_ids.size, dtype=int)\n",
    "    # bhs_mrg_frst = -1 * np.ones(bhs_ids.size, dtype=int)\n",
    "    bhs_bads = np.zeros(bhs_ids.size, dtype=int)\n",
    "\n",
    "    mrg_in_dets = np.zeros_like(mergers[KEYS.ID], dtype=bool)\n",
    "    mrg_in_snaps = np.zeros_like(mrg_in_dets)\n",
    "\n",
    "    bh_count = 0\n",
    "\n",
    "    arr_size = int(bhs_ids.size * (1.0 / TARGET_DELTA_SCA))\n",
    "    print(f\"Guessing array size: {arr_size:.4e}\")\n",
    "\n",
    "    bhs = None\n",
    "    bhs_arr_arr_size = 10000\n",
    "    bhs_arr_ids = []\n",
    "    bhs_arr_loc = []\n",
    "    bhs_arr_num = []\n",
    "    bhs_arr_cnt = 0\n",
    "    bhs_idx = 0\n",
    "\n",
    "    bh_no_snap = []\n",
    "    bh_missing_ids = []\n",
    "    bh_missing_last_scales = []\n",
    "    bh_skipped_ids = []\n",
    "    bh_skipped_snaps = []\n",
    "    bh_bad_next_id = []\n",
    "    # START = 26\n",
    "    START = 0 \n",
    "    \n",
    "    if cosmo is None:\n",
    "        cosmo = illpy_lib.illcosmo.Simulation_Cosmology(SIM_PATH)\n",
    "\n",
    "    first = True\n",
    "    last_snap_scale = 0.0\n",
    "    last_sids = []\n",
    "    for sn in tqdm.trange(START, NUM_SNAPS, desc='snapshots'):\n",
    "        # Load this snapshot data\n",
    "        snap = illpy_lib.illbh.snapshots.Snapshots_Snap(\n",
    "            sn, SIM_PATH, PROC_PATH, recreate=False, verbose=False\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"\\n\\nSnap {sn:03d} size: {snap.size}\")\n",
    "\n",
    "        if len(snap.keys()) == 0:\n",
    "            continue\n",
    "\n",
    "        snap_scale = snap.scale[0]\n",
    "        # Load details for this snapshot\n",
    "        dets = illpy_lib.illbh.details.Details_TNG_Snap(\n",
    "            sn, SIM_PATH, PROC_PATH, recreate=False, verbose=False, cosmo=cosmo\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"dets size: {dets.size}\")\n",
    "        if first and verbose:\n",
    "            print(f\"snap keys:\\n\\t{snap.keys()}\")\n",
    "            print(f\"dets keys:\\n\\t{dets.keys()}\")\n",
    "            first = False\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"snap last: {last_snap_scale:.8e}, scale: {snap.scale[0]:.8e}\")\n",
    "\n",
    "            temp = f\"{cosmo.scale[sn]:.8e}\"\n",
    "            if sn > 0:\n",
    "                temp = f\"{cosmo.scale[sn-1]:.8e}, \" + temp\n",
    "            print(temp)\n",
    "\n",
    "            stats = zmath.stats_str(dets.scale, format=':.8e')\n",
    "            print(f\"dets scales: {stats}\")\n",
    "\n",
    "        # Make sure all details times fall within this snapshot's range\n",
    "        if np.any(dets.scale - last_snap_scale < -1e-4):\n",
    "            err = f\"Details found before previous snapshot!\"\n",
    "            logging.exception(err)\n",
    "            raise RuntimeError(err)\n",
    "\n",
    "        if np.any(snap_scale - dets.scale < -1e-4):\n",
    "            err = f\"Details found after this snapshot!\"\n",
    "            logging.exception(err)\n",
    "            raise RuntimeError(err)\n",
    "\n",
    "        # ---- Go through each unique details BH\n",
    "        sids = snap[KEYS.ID]\n",
    "        dids = dets[KEYS.U_IDS]\n",
    "        # track whether each snapshot BH is found in dets\n",
    "        snap_in_dets = np.zeros(sids.size, dtype=bool)\n",
    "        if verbose:\n",
    "            print(f\"num dets: {dids.size}\")\n",
    "        for bhid, beg, num in zip(dids, dets[KEYS.U_INDICES], dets[KEYS.U_COUNTS]):\n",
    "            if verbose:\n",
    "                print(\"\")\n",
    "            msg = f\"Snap:{sn} dets BHID:{bhid}\"\n",
    "            end = beg + num\n",
    "            # Find the index number out of BHs from all snapshots, maching this one\n",
    "            all_idx = np.argmax(snaps_uids == bhid)\n",
    "            # If this BH is not in any snapshots, skip it\n",
    "            if(snaps_uids[all_idx] != bhid):\n",
    "                bh_no_snap.append([sn, bhid])\n",
    "                logging.debug(f\"{msg} not in any snapshot\")\n",
    "                continue\n",
    "            # Make sure this BH wasn't already marked as bad (shouldn't happen)    \n",
    "            if (bhs_bads[all_idx] != 0):\n",
    "                err = (\n",
    "                    f\"{msg} with {all_idx=}, {bhs_snap_frst[all_idx]=}, \"\n",
    "                    f\"{bhs_snap_last[all_idx]=} - already marked as bad!\"\n",
    "                )\n",
    "                logging.exception(err)\n",
    "                raise RuntimeError(err)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{msg} : {all_idx=}\")            \n",
    "\n",
    "            # -- Store Details\n",
    "            cut = slice(beg, end)\n",
    "            dsca = dets[KEYS.SCALE][cut]\n",
    "            sel = select_at_interval(dsca, TARGET_DELTA_SCA)\n",
    "            num_new = sel.size\n",
    "            # `bhs_idx` is the first index number for this BH's details entries\n",
    "            # `bhs_end` will be the last index number (non-includsive) for this BH's details entries\n",
    "            bhs_end = bhs_idx + num_new\n",
    "\n",
    "            # Expand storage arrays if needed\n",
    "            if bhs_arr_cnt + 1 > len(bhs_arr_ids):\n",
    "                _pad = bhs_arr_arr_size\n",
    "                bhs_arr_ids = np.concatenate([bhs_arr_ids, np.zeros(_pad, dtype=bhs_ids.dtype)])\n",
    "                bhs_arr_loc = np.concatenate([bhs_arr_loc, np.zeros(_pad, dtype=int)])\n",
    "                bhs_arr_num = np.concatenate([bhs_arr_num, np.zeros(_pad, dtype=int)])\n",
    "\n",
    "            bhs_arr_ids[bhs_arr_cnt] = bhid\n",
    "            bhs_arr_loc[bhs_arr_cnt] = bhs_idx\n",
    "            bhs_arr_num[bhs_arr_cnt] = num_new\n",
    "            bhs_arr_cnt += 1\n",
    "\n",
    "            # Make sure `bhs` is large enough to accept new values\n",
    "            if (bhs is None) or (bhs_end > bhs[KEYS.SCALE].size):\n",
    "                bhs = setup_or_expand(bhs, dets, arr_size)\n",
    "\n",
    "            for kk in dets.keys():\n",
    "                if kk in KEYS._DERIVED:\n",
    "                    continue\n",
    "                # Add this BH's entries into the global storage\n",
    "                #    `cut` selects the entries for this BH\n",
    "                #    `sel` selects the subset of entries matching target spacing\n",
    "                bhs[kk][bhs_idx:bhs_end] = dets[kk][cut][sel]\n",
    "\n",
    "            # Increment the counter to the starting point for the next BH\n",
    "            bhs_idx += num_new\n",
    "\n",
    "            # -- Store meta-info\n",
    "\n",
    "            first_snap = (bhs_ids[all_idx] == 0)\n",
    "            # If BH hasnt been stored yet, this is its first snapshot, store first info\n",
    "            if first_snap:\n",
    "                bhs_ids[all_idx] = bhid\n",
    "                bhs_snap_frst[all_idx] = sn\n",
    "                bhs_sca_frst[all_idx] = dets[KEYS.SCALE][beg]\n",
    "            else:\n",
    "                # BH was already stored, make sure it was also found in rpevious snapshot\n",
    "                if bhs_snap_last[all_idx] != sn-1:\n",
    "                    err = f\"{msg} skipped snapshot {sn-1}!\"\n",
    "                    logging.debug(err)\n",
    "                    #print(err)\n",
    "                    #logging.warning(err)\n",
    "                    bh_skipped_ids.append(bhid)\n",
    "                    bh_skipped_snaps.append(sn-1)\n",
    "                    #logging.exception(err)\n",
    "                    #raise RuntimeError(err)\n",
    "\n",
    "                # If this BHID was previously seen in a snapshot but not details, set first scale\n",
    "                if bhs_sca_frst[all_idx] == 0.0:\n",
    "                    bhs_sca_frst[all_idx] = dets[KEYS.SCALE][beg]\n",
    "\n",
    "            # Update the latest scale-factor for this BH\n",
    "            bhs_sca_last[all_idx] = dets[KEYS.SCALE][end-1]\n",
    "\n",
    "            # -- Snapshot: Find the index in this snapshot, matching this (details) BH\n",
    "            snap_idx = np.argmax(sids == bhid)\n",
    "            # Determine if details BH reaches ending snapshot\n",
    "            reaches_end = (sids[snap_idx] == bhid)\n",
    "            if verbose:\n",
    "                print(f\"{msg} : {snap_idx=}, {reaches_end=}\")\n",
    "            if reaches_end:\n",
    "                # Update the last snapshot number this BH was found in\n",
    "                bhs_snap_last[all_idx] = sn\n",
    "                # Update that this snapshot BH was found in details\n",
    "                snap_in_dets[snap_idx] = True\n",
    "\n",
    "            # -- Merger: find the index in mergers this BH participates in\n",
    "            mrg_idx = np.argmax(mrg_uids == bhid)\n",
    "            in_merger = (mrg_uids[mrg_idx] == bhid)\n",
    "            if verbose:\n",
    "                print(f\"{msg} : {mrg_idx=}, {in_merger=}\")\n",
    "\n",
    "            next_bhid = bhid\n",
    "            if in_merger:\n",
    "                # bhs_mrg_frst[all_idx] = mrg_idx   !! NOT SURE WHAT THIS IS DOING !!\n",
    "\n",
    "                # Find the first merger index this BH participates in\n",
    "                #   Note: this could be well-before or well-after this snapshot\n",
    "                mrg_idx = mrg_ulocs[mrg_idx]\n",
    "                # determine if 'in' or 'out' BH in merger\n",
    "                which = (mergers[KEYS.ID][mrg_idx, :] == bhid)\n",
    "                # Store that this merger has been found in details\n",
    "                mrg_in_dets[mrg_idx, which] = True\n",
    "\n",
    "                # Follow the merger tree to find the last merger before this snapshot\n",
    "                nn = tree[KEYS.T_NEXT][mrg_idx]\n",
    "                if verbose:\n",
    "                    print(f\"[{last_snap_scale:.10f}, {snap_scale:.10f}]\")\n",
    "                while (nn > 0) and (mergers[KEYS.SCALE][nn] < snap_scale):\n",
    "                    temp = f\"\\tmm:{nn} - {mergers[KEYS.SCALE][nn]:.10f}\"\n",
    "                    mrg_idx = nn\n",
    "                    nn = tree[KEYS.T_NEXT][nn]\n",
    "                    temp += f\" => mm:{nn} - {mergers[KEYS.SCALE][nn]:.10f}\"\n",
    "                    if verbose:\n",
    "                        print(temp)\n",
    "\n",
    "                # if the merger isn't between the prev and this snapshot, ignore it\n",
    "                msca = mergers[KEYS.SCALE][mrg_idx]\n",
    "                if verbose:\n",
    "                    print(f\"scale:{snap_scale:.10f}, merger:{mrg_idx}, scale:{msca:.10f}\")\n",
    "                if (msca < last_snap_scale) or (msca > snap_scale):\n",
    "                    mrg_idx = None\n",
    "                elif (msca <= last_snap_scale) or (msca >= snap_scale):\n",
    "                    err = \"MERGER AT SNAPSHOT!\"\n",
    "                    logging.exception(err)\n",
    "                    raise RuntimeError(err)\n",
    "\n",
    "            else:\n",
    "                mrg_idx = None\n",
    "\n",
    "            # If no mergers in this snapshot\n",
    "            if mrg_idx is None:\n",
    "                # If BH reaches this snapshot (i.e. end of interval), all is well\n",
    "                if reaches_end:\n",
    "                    continue\n",
    "\n",
    "                # If no mergers, but BH does not reach snapshot end... something wrong!\n",
    "                lvl = logging.INFO\n",
    "                scales = dets[KEYS.SCALE][beg:end]\n",
    "                logging.log(lvl, f\"\\n{msg}\")\n",
    "                logging.log(lvl, f\"snapshot bounds: [{last_snap_scale:.10f}, {snap_scale:.10f}]\")\n",
    "                logging.log(lvl, \"dets scales = \", scales.size, zmath.minmax(scales))\n",
    "                last_scale = np.max(scales)\n",
    "                if bhid in bh_missing_ids:\n",
    "                    raise RuntimeError(f\"{msg} already in missing IDs!!\")\n",
    "\n",
    "                # Store list of BHs that go \"missing\"\n",
    "                bh_missing_ids.append(bhid)\n",
    "                bh_missing_last_scales.append(last_scale)\n",
    "                loc = snaps_ulocs[all_idx]\n",
    "                num = snaps_unums[all_idx]\n",
    "                logging.log(lvl, bhid, snaps_uids[all_idx], loc, num)\n",
    "                scales = zmath.minmax(snaps[KEYS.SCALE][loc:(loc+num)])\n",
    "                logging.log(lvl, \"snap scales range = \", scales)\n",
    "                err = (\n",
    "                    f\"{msg} did not merge in this snapshot but does not reach snapshot end!\"\n",
    "                    f\"  {last_scale=:.10f}\"\n",
    "                )\n",
    "                # Store that this BH has a problem\n",
    "                bhs_bads[all_idx] = 1\n",
    "                logging.info(err)\n",
    "\n",
    "                # If this BH is found in a later snapshot, then something really wrong!\n",
    "                last_snap = snaps[KEYS.SNAP][loc+num-1]\n",
    "                check_bhid = snaps[KEYS.ID][loc+num-1]\n",
    "                if last_snap > sn:\n",
    "                    err = f\"{msg} found in snapshot ({check_bhid=}) {last_snap} > current {sn}!\"\n",
    "                    logging.exception(err)\n",
    "                    raise RuntimeError(err)\n",
    "\n",
    "                # logging.exception(err)\n",
    "                # raise RuntimeError(err)\n",
    "                continue\n",
    "\n",
    "            # At this point, there was a merger in this snapshot\n",
    "\n",
    "            # Get the 'out' BH ID\n",
    "            next_bhid = mergers[KEYS.ID][mrg_idx, BH_TYPE.OUT]\n",
    "            if verbose:\n",
    "                print(f\"{msg} : {next_bhid=}\")\n",
    "\n",
    "            # If this BH (lineage) changed ID numbers\n",
    "            if next_bhid != bhid:\n",
    "                # Make sure the next BH *does* reach the end of this snapshot\n",
    "                # find the snapshot BH matching this ID\n",
    "                snap_idx = np.argmax(sids == next_bhid)\n",
    "                # If the next BH reaches snapshot, all is well\n",
    "                if (sids[snap_idx] == next_bhid):\n",
    "                    continue\n",
    "\n",
    "                # If the new ID is not found in snapshot, problem!\n",
    "                lvl = logging.INFO\n",
    "                # Note that there is an issue with the original (pre-merger) BH\n",
    "                bhs_bads[all_idx] = 2\n",
    "                # NOTE: the next BH `next_bhid` will be marked as problematic once we reach its details entry\n",
    "                logging.log(lvl, f\"\\n{msg}\")\n",
    "                logging.log(lvl, f\"snapshot bounds: [{last_snap_scale:.10f}, {snap_scale:.10f}]\")\n",
    "                logging.log(lvl, f\"{mrg_idx=}, {next_bhid=}\")\n",
    "                # Check if this next-bh is found in *any* snapshots\n",
    "                all_idx = np.argmax(snaps_uids == next_bhid)\n",
    "                if(snaps_uids[all_idx] != next_bhid):\n",
    "                    logging.log(lvl, f\"next_bhid not found in any snapshot!\")\n",
    "                    last_snap = 0\n",
    "                else:\n",
    "                    loc = snaps_ulocs[all_idx]\n",
    "                    num = snaps_unums[all_idx]\n",
    "                    scales = zmath.minmax(snaps[KEYS.SCALE][loc:(loc+num)])\n",
    "                    logging.log(lvl, f\"next_bhid found in scales {scales[0]:.10f}, {scales[1]:.10f}\")\n",
    "                    last_snap = snaps[KEYS.SNAP][loc+num-1]\n",
    "\n",
    "                # Because next-bh is not found in this snapshot, make sure it doesn't occur in later mergers\n",
    "                logging.log(lvl, \"next mergers:\")                \n",
    "                while mrg_idx > 0:\n",
    "                    next_msca = mergers[KEYS.SCALE][mrg_idx]\n",
    "                    logging.log(lvl, f\"\\t{mrg_idx} : {next_msca}\", mergers[KEYS.ID][mrg_idx])\n",
    "                    mrg_idx = tree[KEYS.T_NEXT][mrg_idx]\n",
    "                    logging.log(lvl, f\"\\tnext = {mrg_idx}\")\n",
    "                    if (next_msca > snap_scale) or np.isclose(next_msca, snap_scale, rtol=1e-6, atol=1e-4):\n",
    "                        err = (\n",
    "                            f\"Found another merger {next_msca:.10f} \"\n",
    "                            f\"after missing snapshot scale {snap_scale:.10f}!\"\n",
    "                        )\n",
    "                        raise ValueError(err)\n",
    "\n",
    "                # dids, dets[KEYS.U_INDICES], dets[KEYS.U_COUNTS]\n",
    "                # Find details entries for this next-bh\n",
    "                dets_idx = np.argmax(dids == next_bhid)\n",
    "                last_scale = np.nan\n",
    "                if dids[dets_idx] != next_bhid:\n",
    "                    logging.log(lvl, f\"{next_bhid=} not found in this snapshots details!\")\n",
    "                else:\n",
    "                    loc = dets[KEYS.U_INDICES][dets_idx]\n",
    "                    num = dets[KEYS.U_COUNTS][dets_idx]\n",
    "                    scales = dets[KEYS.SCALE][loc:(loc+num)]\n",
    "                    err = f\"{next_bhid=} found in {scales.size} dets: {scales[0]:.10f}, {scales[-1]:.10f}\"\n",
    "                    logging.log(lvl, err)\n",
    "                    last_scale = scales[-1]\n",
    "\n",
    "                if next_bhid in bh_missing_ids:\n",
    "                    raise RuntimeError(f\"{msg} ==> {next_bhid=} already in missing IDs!!\")\n",
    "\n",
    "                # Store this BH ID as problematic (in this particular way)\n",
    "                bh_bad_next_id.append(next_bhid)\n",
    "                # Beause this (next) BH ID does not reach snapshot end, it will be identified\n",
    "                # later in the details' BHs loop, so we dont need to add it here (i.e. to `bh_missing_ids`)\n",
    "\n",
    "                err = f\"{msg} ==> {next_bhid=}, does not reach snapshot end!\"\n",
    "                logging.info(err)\n",
    "\n",
    "                # next-BH was not found in this snapshot, but IS found in a later one... big problem!\n",
    "                if last_snap > sn:\n",
    "                    err = f\"{msg} ==> {next_bhid=} found in snapshot {last_snap} > current {sn}!\"\n",
    "                    logging.exception(err)\n",
    "                    raise RuntimeError(err)\n",
    "\n",
    "                continue\n",
    "\n",
    "            # At this point, BH was in a merger, but did *not* change ID Number\n",
    "\n",
    "            # If in the snapshot, all is well\n",
    "            if reaches_end:\n",
    "                continue\n",
    "\n",
    "            # BH did *not* reach snapshot end\n",
    "            lvl = logging.INFO\n",
    "            bhs_bads[all_idx] = 3\n",
    "            logging.log(lvl, f\"\\n{msg}\")\n",
    "            logging.log(lvl, f\"snapshot bounds: [{last_snap_scale:.10f}, {snap_scale:.10f}]\")\n",
    "            scales = dets[KEYS.SCALE][loc:(loc+num)]\n",
    "            logging.log(lvl, f\"{scales.size} dets: {scales[0]:.10f}, {scales[-1]:.10f}\")\n",
    "            last_scale = scales[-1]         \n",
    "\n",
    "            # Make sure BH is not found in later snapshot\n",
    "            logging.log(lvl, f\"{mrg_idx=}, {next_bhid=}\")\n",
    "            loc = snaps_ulocs[all_idx]\n",
    "            num = snaps_unums[all_idx]\n",
    "            scales = zmath.minmax(snaps[KEYS.SCALE][loc:(loc+num)])\n",
    "            logging.log(lvl, f\"found in snap scales {scales[0]:.10f}, {scales[1]:.10f}\")\n",
    "            last_snap = snaps[KEYS.SNAP][loc+num-1]\n",
    "            if last_snap > sn:\n",
    "                err = f\"{msg} found in snapshot {last_snap} > current {sn}!\"\n",
    "                raise RuntimeError(err)\n",
    "\n",
    "            # Make sure BH does not participate in later merger\n",
    "            logging.log(lvl, \"next mergers:\")                \n",
    "            while mrg_idx > 0:\n",
    "                next_msca = mergers[KEYS.SCALE][mrg_idx]\n",
    "                logging.log(lvl, f\"\\t{mrg_idx} : {next_msca}\", mergers[KEYS.ID][mrg_idx])\n",
    "                mrg_idx = tree[KEYS.T_NEXT][mrg_idx]\n",
    "                logging.log(lvl, f\"\\tnext = {mrg_idx}\")\n",
    "                if (next_msca > snap_scale) or np.isclose(next_msca, snap_scale, rtol=1e-6, atol=1e-4):\n",
    "                    err = (\n",
    "                        f\"Found another merger {next_msca:.10f} \"\n",
    "                        f\"after missing snapshot scale {snap_scale:.10f}!\"\n",
    "                    )\n",
    "                    raise ValueError(err)\n",
    "\n",
    "            if bhid in bh_missing_ids:\n",
    "                raise RuntimeError(f\"{msg} already in missing IDs!!\")\n",
    "            bh_missing_ids.append(bhid)\n",
    "            bh_missing_last_scales.append(last_scale)\n",
    "\n",
    "            err = f\"{msg} does not reach snapshot end; merges, but does not change ID number!\"\n",
    "            logging.info(err)\n",
    "            #logging.exception(err)\n",
    "            #raise RuntimeError(err)\n",
    "\n",
    "            # } for bhid in details\n",
    "\n",
    "        # Identify any snapshot BHs not found in details\n",
    "        bads = ~snap_in_dets\n",
    "        if np.any(bads):\n",
    "            bads = np.where(bads)[0]\n",
    "            err = f\"{sn=} : {bads.size} snapshot BHs not in details!\"\n",
    "            logging.info(err)\n",
    "\n",
    "            for bb in bads:\n",
    "                bhid = sids[bb]\n",
    "                msg = f\"Snap:{sn} snap BHID:{bhid}\"\n",
    "                all_idx = np.argmax(snaps_uids == bhid)\n",
    "                # BH is in *this* snapshot, make sure it's in *all* snapshots list\n",
    "                if(snaps_uids[all_idx] != bhid):\n",
    "                    bh_no_snap.append([sn, bhid])\n",
    "                    err = f\"{msg} not in all snapshots list!\"\n",
    "                    print(err)\n",
    "                    logging.exception(err)\n",
    "                    raise RuntimeError(err)\n",
    "\n",
    "                # Make sure we're not double counting bad BHs\n",
    "                if (bhs_bads[all_idx] != 0):\n",
    "                    err = f\"{msg} already bad! {all_idx=}, {bhs_snap_frst[all_idx]=}, {bhs_snap_last[all_idx]=}\"\n",
    "                    print(err)\n",
    "                    logging.exception(err)\n",
    "                    raise RuntimeError(err)\n",
    "\n",
    "                first_snap = (bhs_ids[all_idx] == 0)\n",
    "                # Still store the first snapshot this ID is found in, even if problematic\n",
    "                if first_snap:\n",
    "                    bhs_ids[all_idx] = bhid\n",
    "                    bhs_snap_frst[all_idx] = sn\n",
    "                    # bhs_sca_frst[all_idx] = dets[KEYS.SCALE][beg]   # No details to use to set scale\n",
    "                # If BH was already stored, make sure it was found for previous snapshot\n",
    "                elif bhs_snap_last[all_idx] != sn-1:\n",
    "                    err = f\"{msg} skipped snapshot {sn-1}!\"\n",
    "                    logging.warning(err)\n",
    "                    bh_skipped_ids.append(bhid)\n",
    "                    bh_skipped_snaps.append(sn-1)\n",
    "                    logging.exception(err)\n",
    "                    raise RuntimeError(err)\n",
    "\n",
    "                bhs_snap_last[all_idx] = sn\n",
    "\n",
    "        if sn < NUM_SNAPS - 1:\n",
    "            next_scale = cosmo.scale[sn+1]\n",
    "        else:\n",
    "            next_scale = 0.0\n",
    "\n",
    "        for mm, sca in enumerate(mergers[KEYS.SCALE]):\n",
    "            # In merger was before last snapshot, continue\n",
    "            if (sca < last_snap_scale):\n",
    "                continue\n",
    "\n",
    "            if (sca > snap_scale):\n",
    "                break\n",
    "\n",
    "            # If merger is between last and this snapshots, make sure OUT BH is gone\n",
    "            if (mergers[KEYS.ID][mm, BH_TYPE.IN] in sids):\n",
    "                err = f\"Merger {mm} IN BH {mergers[KEYS.ID][mm, BH_TYPE.IN]} found at snapshot {sn} end!\"\n",
    "                logging.exception(err)\n",
    "                raise RuntimeError(err)\n",
    "\n",
    "            # Check if BH ID is in previous snapshot\n",
    "            for bh in range(2):\n",
    "                mrg_in_snaps[mm, bh] = (mergers[KEYS.ID][mm, bh] in last_sids)\n",
    "\n",
    "        # print(f\"{bhs_arr_cnt:.4e} {bhs_idx=:.4e}\")\n",
    "        last_sids = sids\n",
    "        last_snap_scale = snap_scale\n",
    "\n",
    "    meta = dict(\n",
    "        ids=bhs_ids, scale_first=bhs_sca_frst, scale_lst=bhs_sca_last,\n",
    "        snap_first=bhs_snap_frst, snap_last=bhs_snap_last,\n",
    "        \n",
    "        bads=bhs_bads, mrg_in_dets=mrg_in_dets, mrg_in_snaps=mrg_in_snaps,\n",
    "        \n",
    "        no_snap=bh_no_snap, missing_ids=bh_missing_ids, missing_last_scales=bh_missing_last_scales,\n",
    "        skipped_ids=bh_skipped_ids, skipped_snaps=bh_skipped_snaps, bad_next=bh_bad_next_id\n",
    "    )\n",
    "            \n",
    "    # Trim excess elements\n",
    "    for kk, vv in bhs.items():\n",
    "        bhs[kk] = vv[:bhs_idx]\n",
    "\n",
    "    # NOTE: these are invalidated by the lexsort below (? I THINK ?)\n",
    "    bhs_arr_ids = bhs_arr_ids[:bhs_arr_cnt]\n",
    "    bhs_arr_loc = bhs_arr_loc[:bhs_arr_cnt]\n",
    "    bhs_arr_num = bhs_arr_num[:bhs_arr_cnt]\n",
    "\n",
    "    for bh, mis in enumerate(mrg_in_snaps.T):\n",
    "        print(f\"{BH_TYPE.from_value(bh):3s} bad : {zmath.frac_str(~mis, frac_fmt='.3e')}\")\n",
    "\n",
    "    idx = np.lexsort((bhs[KEYS.SCALE], bhs[KEYS.ID]))\n",
    "    for kk, vv in bhs.items():\n",
    "        if kk in KEYS._DERIVED:\n",
    "            continue\n",
    "        bhs[kk] = vv[idx, ...]\n",
    "\n",
    "    u_ids, u_inds, u_counts = np.unique(bhs[KEYS.ID], return_index=True, return_counts=True)\n",
    "    # num_unique = u_ids.size\n",
    "    bhs[KEYS.U_IDS] = u_ids\n",
    "    bhs[KEYS.U_INDICES] = u_inds\n",
    "    bhs[KEYS.U_COUNTS] = u_counts\n",
    "\n",
    "    print(f\"NBH:{u_ids.size:.2e}, entries total: {u_counts.sum():.2e}\"\n",
    "          f\", ave: {np.mean(u_counts):.2e}, med: {np.median(u_counts):.2e}\")\n",
    "\n",
    "    return bhs, meta\n",
    "\n",
    "# RECREATE_BHS = True\n",
    "RECREATE_BHS = False\n",
    "bhs_fname = \"bhs-temp.npz\"\n",
    "bhs_meta_fname = \"bhs-meta-temp.npz\"\n",
    "bhs_fname = os.path.abspath(bhs_fname)\n",
    "bhs_meta_fname = os.path.abspath(bhs_meta_fname)\n",
    "bhs_file_exists = os.path.exists(bhs_fname)\n",
    "if not bhs_file_exists or RECREATE_BHS:\n",
    "    print(f\"BHs file '{bhs_fname}' exists:{bhs_file_exists}!\")\n",
    "    bhs, meta = build_full_bhs(mergers)\n",
    "    np.savez(bhs_fname, **bhs)\n",
    "    np.savez(bhs_meta_fname, **meta)\n",
    "    print(f\"Saved to '{bhs_fname}' size {zio.get_file_size(bhs_fname)}\")\n",
    "    print(f\"Saved to '{bhs_meta_fname}' size {zio.get_file_size(bhs_meta_fname)}\")\n",
    "else:\n",
    "    bhs = np.load(bhs_fname)\n",
    "    meta = np.load(bhs_meta_fname)    \n",
    "    print(f\"Loaded bhs from '{bhs_fname}' and '{bhs_meta_fname}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Different Error Types in BH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make new Merger Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.all(mrg_in_snaps, axis=-1)\n",
    "print(zmath.frac_str(idx))\n",
    "mrg_ids = mergers[KEYS.ID][idx, :]\n",
    "mrg_sca = mergers[KEYS.SCALE][idx]\n",
    "verbose = False\n",
    "new_mergers = {kk: mergers[kk][idx, ...] for kk in mergers.keys() if kk not in KEYS._DERIVED}\n",
    "new_tree = build_tree(new_mergers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_merger_tree(mm, ax=None, follow=True, **kw):\n",
    "    kw.setdefault('alpha', 0.5)\n",
    "    # kw.setdefault('color', '0.5')\n",
    "    # kw.setdefault('s', 100)\n",
    "    kw.setdefault('marker', 'o')\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=[16, 12])\n",
    "    \n",
    "    locs = mrg_locs[mm]\n",
    "    nums = mrg_nums[mm]\n",
    "    \n",
    "    mid = new_mergers[KEYS.ID][mm, BH_TYPE.OUT]\n",
    "    bh = BH_TYPE.REMNANT\n",
    "    ll = locs[bh]\n",
    "    nn = nums[bh]\n",
    "    warnings.warn(\"ignored last remnant element due to `bh_locs` construction error\")\n",
    "    cut = slice(ll, ll+nn-1)\n",
    "    rem_sca = bhs[KEYS.SCALE][cut]\n",
    "    rem_mas = bhs[KEYS.BH_MASS][cut]\n",
    "    ids = bhs[KEYS.ID][cut]    \n",
    "    if not np.all(ids == mid):\n",
    "        print(mm, \"remnant\")\n",
    "        print(mid)\n",
    "        print(ids)\n",
    "        # raise \n",
    "    \n",
    "    for bh in [BH_TYPE.IN, BH_TYPE.OUT]:\n",
    "        ll = locs[bh]\n",
    "        nn = nums[bh]\n",
    "        if nn == 0:\n",
    "            print(f\"EMPTY!  {mm}, {bh}\")\n",
    "            continue\n",
    "            \n",
    "        cut = slice(ll-nn+1, ll+1)\n",
    "        sca = bhs[KEYS.SCALE][cut]\n",
    "        mas = bhs[KEYS.BH_MASS][cut]\n",
    "        ids = bhs[KEYS.ID][cut]    \n",
    "\n",
    "        mids = new_mergers[KEYS.ID][mm, bh]\n",
    "        if not np.all(ids == mids):\n",
    "            print(mids)\n",
    "            print(ids)\n",
    "            raise\n",
    "            \n",
    "        # hh = ax.scatter(sca, mas, **kw)\n",
    "        hh, = ax.plot(sca, mas, **kw)\n",
    "        temp = dict(kw)\n",
    "        # temp['color'] = hh.get_fc()[0]\n",
    "        temp['color'] = hh.get_color()\n",
    "        dm = np.diff(mas)\n",
    "        if np.any(dm < 0.0):\n",
    "            raise\n",
    "        ax.plot(zmath.midpoints(sca), dm, **temp)\n",
    "        \n",
    "        temp['marker'] = None\n",
    "        hh, = ax.plot([sca[-1], rem_sca[0]], [mas[-1], rem_mas[0]], **temp)\n",
    "        \n",
    "    nxt = new_tree[KEYS.T_NEXT][mm]\n",
    "    if (nxt > 0) and ((follow is True) or (follow > 0)):\n",
    "        print(f\"{mm} ==> {nxt}\")\n",
    "        if follow is not True:\n",
    "            follow = follow - 1\n",
    "            print(f\"{follow=}\")\n",
    "        draw_merger_tree(nxt, ax=ax, follow=follow, **kw)\n",
    "    else:          \n",
    "        temp = dict(kw)\n",
    "        temp['color'] = hh.get_color()\n",
    "        ax.plot(rem_sca, rem_mas, **temp)\n",
    "        ax.plot(zmath.midpoints(rem_sca), np.diff(rem_mas), **temp)\n",
    "        \n",
    "    return ax\n",
    "\n",
    "# Nice mergers: [-800, -1000]\n",
    "# Missing merger in BHs: [115596010125]\n",
    "MRG_NUM = -1030\n",
    "# MRG_NUM = 20434\n",
    "ax = draw_merger_tree(MRG_NUM, follow=0)\n",
    "ax.set(yscale='log') # , ylim=[1e-3, 1e-1], xlim=[0.7, 1.0])\n",
    "plt.show()\n",
    "    \n",
    "print(new_mergers[KEYS.ID][MRG_NUM, BH_TYPE.IN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tree[KEYS.T_PREV][-1030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beg_end_for_id(bhid):\n",
    "    idx = np.argmax(bhs[KEYS.U_IDS] == bhid)\n",
    "    if bhs[KEYS.U_IDS][idx] != bhid:\n",
    "        raise ValueError(\"BH ID not in values!\")\n",
    "        \n",
    "    beg = bhs[KEYS.U_INDICES][idx]\n",
    "    end = bhs[KEYS.U_COUNTS][idx]\n",
    "    end += beg\n",
    "    return beg, end\n",
    "\n",
    "mrg_ids = new_mergers[KEYS.ID]\n",
    "mrg_sca = new_mergers[KEYS.SCALE]\n",
    "num_tot = bhs[KEYS.SCALE].size\n",
    "\n",
    "mrg_locs = -1 * np.ones((mrg_sca.size, 3), dtype=int)\n",
    "mrg_nums = -1 * np.ones((mrg_sca.size, 3), dtype=int)\n",
    "for mm, (ids, msca) in tqdm.tqdm(enumerate(zip(mrg_ids, mrg_sca)), total=mrg_sca.size, desc='mergers'):\n",
    "    if verbose:\n",
    "        print()\n",
    "    for bh in [BH_TYPE.IN, BH_TYPE.OUT]:\n",
    "        bhid = ids[bh]\n",
    "        msg = f\"{mm} : {bh} ({BH_TYPE.from_value(bh)}), {bhid} @ {msca:.8f}\"\n",
    "        beg, end = beg_end_for_id(bhid)\n",
    "        dsca = bhs[KEYS.SCALE][beg:end]\n",
    "        dmss = bhs[KEYS.BH_MASS][beg:end]\n",
    "        if dsca.size < 2:\n",
    "            raise ValueError(f\"{msg} :: {dsca.size=}!\")\n",
    "            \n",
    "        dm = np.diff(dmss)\n",
    "        if np.any((dm < 0.0) & (~np.isclose(0.0, dm/dmss[:-1], atol=1e-3, rtol=0.0))):\n",
    "            print(msg)\n",
    "            print(zmath.stats_str(dm, format=':.8e'))\n",
    "            bads = np.where(dm < 0.0)[0]\n",
    "            print(f\"{bads.size=} {bads=}\")\n",
    "            print(zmath.stats_str(dm[bads], format=\".8e\"))\n",
    "            \n",
    "            bb = bads[0]\n",
    "            print(f\"dm = {dm[bb]:.8e} :: frac={dm[bb]/dmss[bb]:.8e}\")\n",
    "            cut = slice(np.max([bb-1, 0]), np.min([bb+3, dm.size]))\n",
    "            print(f\"dm   [{cut}] = \", zmath.str_array(dm[cut], format=':.8e'))\n",
    "            print(f\"mass [{cut}] = \", zmath.str_array(dmss[cut], format=':.8e'))\n",
    "            print(f\"scale[{cut}] = \", zmath.str_array(dsca[cut], format=':.8f'))\n",
    "            print(f\"id   [{cut}] = \", zmath.str_array(bhs[KEYS.ID][beg:end][cut], format=':d'))\n",
    "            raise ValueError(f\"{msg} :: found decreasing mass!\")\n",
    "        \n",
    "        if msca < dsca[0]:\n",
    "            raise ValueError(f\"{msg} :: First detail ({dsca[0]:.8f}) is after merger ({msca:.8f})!\")\n",
    "\n",
    "        if bh == BH_TYPE.IN:\n",
    "            idx = dsca.size - 1\n",
    "            if dsca[idx] > msca:\n",
    "                raise            \n",
    "            # mrg_locs[mm, bh] = beg + idx\n",
    "        else:\n",
    "            # Find the index for the first detail *equal-to or past* the merger time\n",
    "            idx = np.argmax(msca < dsca)\n",
    "            # If *past* merger time, then the last before-merger entry is the previous one (subtract one)\n",
    "            if dsca[idx] > msca:\n",
    "                if idx == 0:\n",
    "                    raise\n",
    "                # mrg_locs[mm, bh] = beg + idx - 1\n",
    "                # mrg_locs[mm, BH_TYPE.REMNANT] = beg + idx\n",
    "                idx = idx - 1\n",
    "            # If *equal-to* merger time, this detail is the last before-merger entry (do not modify)\n",
    "            elif dsca[idx] == msca:\n",
    "                if idx >= dsca.size - 1:\n",
    "                    raise\n",
    "                # mrg_locs[mm, bh] = beg + idx\n",
    "                # mrg_locs[mm, BH_TYPE.REMNANT] = beg + idx + 1\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # Convert back to global index number, and store\n",
    "        loc = beg + idx\n",
    "        mrg_locs[mm, bh] = loc\n",
    "        # If this is the 'out' BH, we know the remnant starts at the subsequent index number\n",
    "        if bh == BH_TYPE.OUT:\n",
    "            mrg_locs[mm, BH_TYPE.REMNANT] = loc + 1\n",
    "\n",
    "        if bhs[KEYS.SCALE][loc] > msca:\n",
    "            raise ValueError(f\"{msg} : loc:{loc}, sca:{bhs[KEYS.SCALE][loc]:.8f}\")\n",
    "            \n",
    "        if verbose:\n",
    "            lo = np.max([idx-2, 0])\n",
    "            hi = np.min([idx+2, dsca.size-1])\n",
    "            print(f\"{msg} : {loc=}, a={bhs[KEYS.SCALE][loc]:.8f}\")\n",
    "            print(f\"\\t{lo=}, {loc-beg}, {hi=} ({dsca.size})\")       \n",
    "            print(f\"\\ta = \", zmath.str_array(dsca[lo:hi], format=':.8f'))\n",
    "            print(f\"\\tm = \", zmath.str_array(dmss[lo:hi], format=':.8e'))\n",
    "            print(f\"\\tdm  = \", zmath.str_array(np.diff(dmss[lo:hi]), format=':.8e'))\n",
    "        \n",
    "        # -- Find the previous number of entries for this BH (after previous merger) --\n",
    "        prev = new_tree[KEYS.T_PREV][mm, bh]\n",
    "        # If there was no previous merger, then all entries apply (`num = idx + 1`)\n",
    "        if prev < 0:\n",
    "            nprev = idx + 1\n",
    "        # If there was a previous merger, count entries after that\n",
    "        else:\n",
    "            prev_msca = new_mergers[KEYS.SCALE][prev]\n",
    "            # Count the entries until merger that are after (or equal to) previous merger\n",
    "            #    details entries at the same time as merger occur before the merger (hence greater-*or-equal*)\n",
    "            #    count inclusively the last details entry (hence `idx+1`)\n",
    "            nprev = np.count_nonzero(dsca[:idx+1] >= prev_msca)\n",
    "            # The entry at `idx-nprev` should be outside of the above; i.e. before the merger\n",
    "            if (idx > nprev) and (dsca[idx-nprev] >= prev_msca):\n",
    "                raise ValueError(f\"{msg} :: Failed to find the correct number {nprev=} of pre-merger entries!\")\n",
    "            # The entry at `idx-nprev+1` should be *inside* of the above; i.e. after-or-at the merger\n",
    "            if dsca[idx-nprev+1] < prev_msca:\n",
    "                raise ValueError(f\"{msg} :: Failed to find the correct number {nprev=} of pre-merger entries!\")\n",
    "            if np.any(bhs[KEYS.ID][loc-nprev+1:loc+1] != bhid):\n",
    "                raise ValueError(f\"{msg} :: One of the previous {nprev=} details has the wrong ID number!\")\n",
    "\n",
    "        mrg_nums[mm, bh] = nprev\n",
    "        \n",
    "        # -- Consider the remnant BH --\n",
    "        if bh == BH_TYPE.IN:\n",
    "            continue\n",
    "        \n",
    "        # -- Find the following number of entries for this BH (before subsequent merger) --\n",
    "        next = new_tree[KEYS.T_NEXT][mm]\n",
    "        # If there is no next merger, then all entries apply (`num = idx + 1`)\n",
    "        if next < 0:\n",
    "            nnext = (end - beg) - idx\n",
    "        # If there was a previous merger, count entries after that\n",
    "        else:\n",
    "            next_msca = new_mergers[KEYS.SCALE][next]\n",
    "            # Count the entries until merger that are after (or equal to) previous merger\n",
    "            #    details entries at the same time as merger occur before the merger (hence greater-*or-equal*)\n",
    "            #    count inclusively the last details entry (hence `idx+1`)\n",
    "            nnext = np.count_nonzero(dsca[idx:] <= next_msca)\n",
    "            # The entry at `idx+nnext` should be outside of the above; i.e. after the next merger\n",
    "            # print(f\"{next_msca=:.8f}, {idx=}, {nnext=}\")\n",
    "            # print(zmath.str_array(dsca[idx:idx+nnext+2], format=':.8f'))\n",
    "            # print(zmath.str_array(dsca[idx:idx+nnext+2] <= next_msca, format=':10d'))\n",
    "            if (loc+nnext < num_tot) and ((bhs[KEYS.SCALE][loc+nnext] <= next_msca) and (bhs[KEYS.ID][loc+nnext] == bhid)):\n",
    "                raise ValueError(f\"{msg} :: Failed to find the correct number {nnext=} of post-merger entries!\")\n",
    "            # The entry at `idx+nnext-1` should be *inside* of the above; i.e. after the next merger\n",
    "            if bhs[KEYS.SCALE][loc+nnext-1] > next_msca:\n",
    "                raise ValueError(f\"{msg} :: Failed to find the correct number {nnext=} of post-merger entries!\")\n",
    "            if np.any(bhs[KEYS.ID][loc:loc+nnext] != bhid):\n",
    "                raise ValueError(f\"{msg} :: One of the following {nnext=} details has the wrong ID number!\")\n",
    "\n",
    "        mrg_nums[mm, BH_TYPE.REMNANT] = nnext\n",
    "\n",
    "    loc = mrg_locs[mm, 2]\n",
    "    if verbose:\n",
    "        print(f\"{msg} : {loc=}, a={bhs[KEYS.SCALE][loc]:.8f}\")\n",
    "    if (bhs[KEYS.SCALE][loc] <= msca) or (bhs[KEYS.SCALE][loc-1] > msca):\n",
    "        raise ValueError()\n",
    "\n",
    "    test_masses = [bhs[KEYS.BH_MASS][ll] for ll in mrg_locs[mm]]\n",
    "    test_remn = np.sum(test_masses[:2])\n",
    "    test_err = (test_masses[-1] - test_remn) / np.mean(test_masses[:2])\n",
    "    if (test_err < 0.0) and not np.isclose(test_remn, test_masses[-1], rtol=1e-5, atol=0):\n",
    "        print(msg)\n",
    "        print(test_masses)\n",
    "        print(f\"{test_remn:.8f}, {test_masses[-1]:.8f} :: {test_err:.8e}\")\n",
    "        print()\n",
    "        raise ValueError(f\"{msg} :: Total mass of remnant too low!\")\n",
    "                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zmath.frac_str(bhs_mrg_frst >= 0, frac_fmt='.4e')\n",
    "zmath.frac_str(mrg_in_dets > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bhs_ids = np.zeros_like(snaps_uids)\n",
    "# bhs_sca_frst = np.zeros(bhs_ids.size)\n",
    "# bhs_sca_last = np.zeros(bhs_ids.size)\n",
    "# bhs_snap_frst = -1 * np.ones(bhs_ids.size, dtype=int)\n",
    "# bhs_snap_last = -1 * np.ones(bhs_ids.size, dtype=int)\n",
    "# bhs_mrg_frst = -1 * np.ones(bhs_ids.size, dtype=int)\n",
    "# bhs_bads = np.zeros(bhs_ids.size, dtype=int)\n",
    "\n",
    "# mrg_in_dets = np.zeros_like(mergers[KEYS.ID], dtype=bool)\n",
    "# mrg_in_snaps = np.zeros_like(mrg_in_dets)\n",
    "\n",
    "# bh_count = 0\n",
    "\n",
    "# bhs = dict()\n",
    "\n",
    "# bh_no_snap = []\n",
    "# bh_missing_ids = []\n",
    "# bh_missing_last_scales = []\n",
    "# bh_skipped_ids = []\n",
    "# bh_skipped_snaps = []\n",
    "# bh_bad_next_id = []\n",
    "\n",
    "\n",
    "print(f\"Merger BHs in dets : {zmath.frac_str(mrg_in_dets > 0)}\")\n",
    "print(f\"              both : {zmath.frac_str(np.all(mrg_in_dets > 0, axis=1))}\")\n",
    "print(f\"Merger BHs in snaps: {zmath.frac_str(mrg_in_snaps > 0)}\")\n",
    "print(f\"              both : {zmath.frac_str(np.all(mrg_in_snaps > 0, axis=1))}\")\n",
    "print(f\"Tree snaps: {zmath.frac_str(tree[KEYS.SNAP] > 0)}, all: {np.all(tree[KEYS.SNAP] > 0)}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"BHs with details and no snapshots: {len(bh_no_snap)}\")\n",
    "print(f\"BHs that go missing: {len(bh_missing_ids)}\")\n",
    "if len(bh_missing_ids) > 0:\n",
    "    fig, ax = plt.subplots(figsize=[8, 4])\n",
    "    ax.hist(bh_missing_last_scales, bins=80, density=True)\n",
    "    ax.plot(*kale.pdf(bh_missing_last_scales))\n",
    "    kale.plot.draw_carpet(bh_missing_last_scales, ax=ax)\n",
    "    xx = bh_missing_last_scales\n",
    "    yy = np.random.normal(-1.0, 0.5, len(xx))\n",
    "#     ax.scatter(xx, yy, color='r', alpha=0.25)\n",
    "    \n",
    "unique_missing = np.unique(bh_missing_ids)\n",
    "if unique_missing.size != len(bh_missing_ids):\n",
    "    print(f\"Unique missing BHs is {unique_missing.size}!\")\n",
    "    \n",
    "print(f\"BHs skipping snapshots: {len(bh_skipped_ids)}\")\n",
    "unique_skipped = np.unique(bh_skipped_ids)\n",
    "if unique_skipped.size != len(bh_skipped_ids):\n",
    "    print(f\"Unique skipped BHs is {unique_skipped.size}\")\n",
    "        \n",
    "print(\"\\n\")\n",
    "        \n",
    "        \n",
    "bads = np.where(bhs_ids == 0)[0]\n",
    "print(f\"Empty `bhs_ids`: {bads.size} : {bads=}\")\n",
    "\n",
    "print(f\"Bad BHs: {zmath.frac_str(bhs_bads > 0)}\")\n",
    "print(zmath.stats_str(bhs_bads))\n",
    "for ii in range(1, 4):\n",
    "    bads = np.where(bhs_bads == ii)[0]\n",
    "    print(f\"{ii} - {bads.size}: {bads=}\")\n",
    "\n",
    "sel = (bhs_ids > 0)\n",
    "\n",
    "bads = (bhs_sca_frst[sel] > bhs_sca_last[sel])\n",
    "if np.any(bads):\n",
    "    bads = np.where(bads)[0]\n",
    "    print(f\"sca first after last!  {bads.size} : {bads=}\")\n",
    "    print(bhs_sca_frst[sel][bads], bhs_sca_last[sel][bads])\n",
    "    \n",
    "bads = (bhs_snap_frst[sel] > bhs_snap_last[sel])\n",
    "if np.any(bads):\n",
    "    bads = np.where(bads)[0]\n",
    "    print(f\"snap first after last!  {bads.size} : {bads=}\")\n",
    "    print(bhs_snap_frst[sel][bads], bhs_snap_last[sel][bads])\n",
    "    \n",
    "    \n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Bad Merger locations:\", zmath.frac_str(mrg_locs < 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = 1e-4\n",
    "zbeg = 5.0\n",
    "\n",
    "abeg = cosmo._z_to_a(zbeg)\n",
    "aend = 1.0\n",
    "print(f\"a: [{abeg:.6f}, {aend:.6f}]\")\n",
    "\n",
    "aa = aend - da * np.arange(0, (aend//da)+2)\n",
    "aa = aa[::-1]\n",
    "aa = aa[(aa >= abeg)]\n",
    "\n",
    "tage = cosmo.age(cosmo._a_to_z(aa)).to('Myr').value\n",
    "dt = np.diff(tage)\n",
    "ta = zmath.midpoints(tage)\n",
    "plt.plot(ta, dt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_A = 1e-4\n",
    "# DELTA_A = 3e-3\n",
    "\n",
    "def select_at_interval_one(xx, dx):\n",
    "    beg = np.min(xx)\n",
    "    end = np.max(xx)\n",
    "    num = (end - beg) / dx\n",
    "    num = np.int(np.ceil(num))\n",
    "    # Array of optimal spacings\n",
    "    yy = beg + dx * np.arange(num+1)    \n",
    "    # Find locations in input nearest to the optimal spacings\n",
    "    sel = zmath.argnearest(xx, yy, assume_sorted=True)\n",
    "    # Also include edges\n",
    "    sel = np.concatenate([[0], sel, [xx.size-1]])\n",
    "    # Make sure results are all unique\n",
    "    sel = np.unique(sel)\n",
    "    return sel\n",
    "    \n",
    "def select_at_interval_two(xx, target):\n",
    "    sel = np.zeros_like(xx, dtype=bool)\n",
    "    sel[0] = True\n",
    "    sel[-1] = True\n",
    "\n",
    "    dx = np.concatenate([[0.0], np.diff(xx)])\n",
    "    \n",
    "    idx = 1\n",
    "    step = 0.0\n",
    "    while idx < xx.size - 1:\n",
    "        step += dx[idx]\n",
    "        if step > target:\n",
    "            if sel[idx-1]:\n",
    "                sel[idx] = True\n",
    "                step = 0.0\n",
    "            else:\n",
    "                sel[idx-1] = True\n",
    "                step = dx[idx]\n",
    "            \n",
    "        idx += 1\n",
    "    \n",
    "    return sel\n",
    "    \n",
    "abeg = np.random.uniform(0.0, 0.5)\n",
    "\n",
    "num = int(10.0 / DELTA_A)\n",
    "da = zmath.log_normal_base_10(DELTA_A, 0.5, num)\n",
    "aa = np.concatenate([[abeg], da])\n",
    "aa = np.cumsum(aa)\n",
    "aa = np.concatenate([aa[(aa < 1.0)], [1.0]])\n",
    "print(f\"{aa.size=}\")\n",
    "\n",
    "target = DELTA_A * 10.0\n",
    "TIMES = 1e3\n",
    "\n",
    "functions = [select_at_interval_one, select_at_interval_two]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=[16, 6], ncols=len(functions) + 1)\n",
    "\n",
    "for ax in axes[:-1]:\n",
    "    ax.scatter(np.arange(aa.size), aa, c='0.5', alpha=0.5, s=100)\n",
    "\n",
    "ax = axes[-1]\n",
    "ax.hist(np.log10(np.diff(aa)), bins='auto', color='0.5', alpha=0.5, density=True)\n",
    "ax.axvline(np.log10(target), color='b', ls='--', alpha=0.5)\n",
    "\n",
    "for jj, func in enumerate(functions):\n",
    "    beg = datetime.datetime.now()\n",
    "    for ii in range(int(TIMES)):\n",
    "        sel = func(aa, target)\n",
    "\n",
    "    dur = datetime.datetime.now() - beg\n",
    "    print(f\"Duration over {TIMES} tests: {dur.total_seconds():.4e}\")\n",
    "        \n",
    "    bb = aa[sel]\n",
    "    *_, hh = axes[-1].hist(np.log10(np.diff(bb)), bins='auto', density=True, alpha=0.5)\n",
    "    color = hh[0].get_fc()\n",
    "    \n",
    "    cc = np.ones_like(aa) * np.nan\n",
    "    cc[sel] = aa[sel]\n",
    "    hh = axes[jj].scatter(np.arange(cc.size), cc, color=color, alpha=0.5, s=100, marker='*')\n",
    "    for c in cc:\n",
    "        axes[jj].axhline(c, color=color)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_A = 4e-3\n",
    "\n",
    "def select_at_interval(xx, dx):\n",
    "    beg = np.min(xx)\n",
    "    end = np.max(xx)\n",
    "    num = (end - beg) / dx\n",
    "    num = np.int(np.ceil(num))\n",
    "    yy = beg + dx * np.arange(num+1)\n",
    "    print(f\"xx={zmath.stats_str(xx)}\")\n",
    "    print(f\"dx={zmath.stats_str(np.diff(xx))}\")\n",
    "    print(f\"yy={zmath.stats_str(yy)}\")\n",
    "    print(f\"dy={zmath.stats_str(np.diff(yy))}\")\n",
    "    print(f\"{xx.size=}, {yy.size=}\")\n",
    "    \n",
    "    sel = zmath.argnearest(xx, yy, assume_sorted=True)\n",
    "    sel = np.concatenate([[0], sel, [xx.size-1]])\n",
    "    sel = np.unique(sel)\n",
    "    \n",
    "    # print(f\"{zmath.frac_str(sel)}\")\n",
    "    return sel\n",
    "    \n",
    "abeg = np.random.uniform(0.0, 0.5)\n",
    "\n",
    "num = int(10.0 / DELTA_A)\n",
    "da = zmath.log_normal_base_10(DELTA_A, 0.5, num)\n",
    "aa = np.concatenate([[abeg], da])\n",
    "aa = np.cumsum(aa)\n",
    "aa = np.concatenate([aa[(aa < 1.0)], [1.0]])\n",
    "\n",
    "target = DELTA_A * 10.0\n",
    "sel = select_at_interval(aa, target)\n",
    "bb = aa[sel]\n",
    "\n",
    "fig, axes = plt.subplots(figsize=[16, 6], ncols=2)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.scatter(np.arange(aa.size), aa, c='r', alpha=0.5, s=100)\n",
    "cc = np.ones_like(aa) * np.nan\n",
    "cc[sel] = aa[sel]\n",
    "ax.scatter(np.arange(cc.size), cc, c='b', alpha=0.5, s=100, marker='*')\n",
    "\n",
    "ax = axes[1]\n",
    "ax.hist(np.log10(np.diff(aa)), bins='auto', color='r', alpha=0.5)\n",
    "ax.hist(np.log10(np.diff(bb)), bins='auto', color='b', alpha=0.5)\n",
    "ax.axvline(np.log10(target), color='b', ls='--', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = 100000851369\n",
    "bad = np.where(snaps[KEYS.U_IDS] == tt)[0][0]\n",
    "print(tt, bad)\n",
    "loc = snaps[KEYS.U_INDICES][bad]\n",
    "num = snaps[KEYS.U_COUNTS][bad]\n",
    "print(loc, num)\n",
    "cut = slice(loc, loc+num)\n",
    "snap_nums = snaps[KEYS.SNAP][cut]\n",
    "snap_scas = snaps[KEYS.SCALE][cut]\n",
    "print(snap_nums)\n",
    "print(snap_scas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dets_32 = illpy_lib.illbh.details.Details_TNG_Snap(32, SIM_PATH, PROC_PATH, recreate=False, verbose=False, cosmo=cosmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = np.where(dets_32[KEYS.U_IDS] == tt)[0][0]\n",
    "print(tt, bad)\n",
    "loc = dets_32[KEYS.U_INDICES][bad]\n",
    "num = dets_32[KEYS.U_COUNTS][bad]\n",
    "print(loc, num)\n",
    "cut = slice(loc, loc+num)\n",
    "snap_nums = dets_32[KEYS.ID][cut]\n",
    "snap_scas = dets_32[KEYS.SCALE][cut]\n",
    "print(snap_nums)\n",
    "print(snap_scas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
